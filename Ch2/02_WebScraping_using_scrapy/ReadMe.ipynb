{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a book scraper using Scrapy\n",
    "\n",
    "Scrapy is a python framework for scraping and crawling websites. This tutorial demonstrates the use\n",
    "of scrapy to quickly mine a large amount of data from a demo website\n",
    "[books.toscrape.com](http://books.toscrape.com/).\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install Scrapy\n",
    "```\n",
    "\n",
    "Refer the [documentation](https://docs.scrapy.org/en/latest/intro/install.html) for advanced\n",
    "installation instructions.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "### Set-Up\n",
    "\n",
    "First, we need to start a new scrapy project.\n",
    "\n",
    "```\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "\n",
    "Scrapy auto-generates a few boilerplate files. The directory structure should look like the\n",
    "following.\n",
    "\n",
    "```\n",
    ".\n",
    "└── tutorial\n",
    "    ├── scrapy.cfg\n",
    "    └── tutorial\n",
    "        ├── __init__.py\n",
    "        ├── items.py\n",
    "        ├── middlewares.py\n",
    "        ├── pipelines.py\n",
    "        ├── settings.py\n",
    "        └── spiders\n",
    "            └── __init__.py\n",
    "```\n",
    "\n",
    "Now enter the base project directory `cd tutorial`.\n",
    "\n",
    "### A Spider\n",
    "\n",
    "We will now create a new spider inside `tutorial/spiders`. A spider is nothing but a python class\n",
    "which inherits from the `scrapy.Spider` class. Each spider has a unique `name` which is used when\n",
    "running the spider and a `start_urls` which a list of URLs the spider starts crawling as soon as it\n",
    "is run; we will set `start_urls` to `[\"http://books.toscrape.com/\"]` which is the website we want to\n",
    "scrape. Create a new file at `tutorial/spiders/books_spider.py` which will have our spider, with the\n",
    "following content.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books\"\n",
    "\n",
    "    start_urls = [\"http://books.toscrape.com/\"]\n",
    "```\n",
    "\n",
    "When a spider is run, it starts by executing the `parse` function of it on all the `start_urls`. The\n",
    "`parse` function has a parameter named `response` which stores all information about the fetched\n",
    "page. `response.url` contains URL of the currently fetched page. Lets print the URL in the fetch\n",
    "function and see what it does. (The following code should be inside the BookSpider defined earlier.\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        print(response.url)\n",
    "```\n",
    "\n",
    "Let's now run the spider. Go to base project directory (`tutorial`) and run\n",
    "(Note that `books` is the name we gave the spider inside the BookSpider class)\n",
    "```bash\n",
    "scrapy crawl books\n",
    "```\n",
    "You will see lots of text output, try to find `http://books.toscrape.com/` in one of the lines.\n",
    "\n",
    "### CSS and XPath selectors\n",
    "\n",
    "Now, lets focus on actually extracting the content out of the website. If you have ever used CSS,\n",
    "you will know about selectors which are used to apply style to specific elements. We can use the\n",
    "same type of selectors to extract data from specific elements. This can be done by using\n",
    "`response.css(\".css selector here\")` and then use `getall()` or `get()` function of the CSS object\n",
    "to actually get the element. You can use `::text` in the selector to get the text contained in the\n",
    "required element. To get a specific property of the element, say `href`, use `attrib[\"href\"]` on the\n",
    "object returned by `response.css`.\n",
    "\n",
    "A more powerful type of selectors are XPath selectors. [Here](https://devhints.io/xpath) is\n",
    "a cheat-sheet for XPath and [here](https://www.guru99.com/xpath-selenium.html) is a more complete\n",
    "tutorial. A summary of XPath selectors that will be used in this tutorial are:\n",
    " - `//element`: Get all elements of type `element` anywhere in the DOM.\n",
    " - `element[@class='classname']]`: Get only elements of type `element` and of class `classname`.\n",
    "   Note the class is matched entirely, if a element is of class `class1 class2`, a `@class='class1'`\n",
    "   will not match it.\n",
    " - `element[contains(@class, 'classname')]`: Get elements of type `element` whose `class` contains\n",
    "   `classname` anywhere.\n",
    " - `element/text()`: Get the text contained inside elements.\n",
    " - `element/@href`: Get the `href` attribute (can be replace with any attribute)\n",
    " - `element1/element2`: Get `element2` which a direct child of `element1`\n",
    "\n",
    "### Extracting Data\n",
    "\n",
    "Let's take a look at the website we want to scrape. It has a sidebar which shows the list of all\n",
    "categories and on the right we can see all books.\n",
    "\n",
    "The approach used in this tutorial is going to each of the categories and scraping all books inside\n",
    "them.\n",
    "\n",
    "A great tool to test and experiment with selectors in real time is the **Scrapy shell**. Launch the\n",
    "scrapy shell for website of interest using the following command.\n",
    "\n",
    "```bash\n",
    "scrapy shell \"http://books.toscrape.com/\"\n",
    "```\n",
    "(Pro tip: have IPython installed to get a better scrapy shell experience)\n",
    "\n",
    "From the shell, you can view the exact response it received using `view(response)` (make sure you do\n",
    "this for any website you want to scrape before proceeding, the website may look different to your\n",
    "browser and scrapy since scrapy does not have JavaScript by default).\n",
    "\n",
    "Now, let's see how to select the list of categories. We will use the ever useful *Inspect Element*\n",
    "tool. It can be seen in the source that the category list is in a `div` of class `side_categories`.\n",
    "This div contains an unordered list whose only element contains another unordered list! The second\n",
    "`ul` is the one we want since each element of this contains an anchor tag with the URL to each\n",
    "category. Thus, the XPaths are as follows (you can experiment with XPaths in the scrapy shell till\n",
    "you get the output you need).\n",
    "\n",
    "```python\n",
    "        cat_names = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/text()\").getall()\n",
    "        cat_urls = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/@href\").getall()\n",
    "```\n",
    "`cat_names` stores the list of all categories and `cat_urls` the corresponding URLs. We can iterate\n",
    "over these using zip. There is one issue though, the URLs are relative to the current URL; to fix\n",
    "this, we use `response.urljoin` to get the absolute URL. Now that we have the URLs, we need a way to\n",
    "scrape them separately, the `parse` function is specifically made to get the list of categories;\n",
    "hence, we need a separate function which will parse all books in a category - we call this function\n",
    "`parse_category`. To tell scrapy to parse a particular URL, we need to create an object of\n",
    "`scrapy.Request` and return it. Since we have a list of URLs to return, we'll use `yield` instead of\n",
    "return to return multiple Requests to scrapy. A `scrapy.Request` object required two parameters -\n",
    "the URL and the function to pass the handle to after a response is received, called `callback`. We\n",
    "  use another parameter `cb_kwargs` to pass additional parameters to the callback function instead\n",
    "of just the response. Here is the `parse` function after adding all the above mentioned features.\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        num_cats_to_parse = 5\n",
    "        cat_names = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/text()\").getall()\n",
    "        cat_urls = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/@href\").getall()\n",
    "        for _, name, url in zip(range(num_cats_to_parse), cat_names, cat_urls):\n",
    "            name = name.strip()\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url,\n",
    "                                 callback=self.parse_category,\n",
    "                                 cb_kwargs=dict(cat_name=name))\n",
    "```\n",
    "Since there are quite a few categories, we limit the number of categories crawled.…\n",
    "\n",
    "Now, let's scrape all the books in a category which will be done inside the `parse_category`\n",
    "function. You can use scrapy shell again on one of the category URLs to build selectors\n",
    "interactively. Here, we can see that each book is inside an `article` of class `product_pod`, which\n",
    "has a `h3` containing an anchor tag linking to the book's URL. Thus, the line to get all books will\n",
    "be as follows.\n",
    "\n",
    "```python\n",
    "        book_urls = response.xpath(\"//article[@class='product_pod']/h3/a/@href\").getall()\n",
    "```\n",
    "Now we can loop through the book URLs and `yield` a `scrapy.Request` for each URL with the callback\n",
    "as a new function `parse_book` which we will define later. You would have noticed that some\n",
    "categories have too many books to be displayed in one page and as such they are paginated i.e.,\n",
    "separated into pages and there's a `next` button near the bottom of the page. After `yield`ing all\n",
    "the requests for books we find the `next` button and get its URL which is then used to `yield`\n",
    "another request but with the callback being `parse_category` (which is nothing but a recursion).\n",
    "The entire code for this function is as follows.\n",
    "\n",
    "```python\n",
    "    def parse_category(self, response, cat_name):\n",
    "        book_urls = response.xpath(\"//article[@class='product_pod']/h3/a/@href\").getall()\n",
    "\n",
    "        for book_url in book_urls:\n",
    "            book_url = response.urljoin(book_url)\n",
    "            yield scrapy.Request(book_url, callback=self.parse_book,\n",
    "                                 cb_kwargs=dict(cat_name=cat_name))\n",
    "\n",
    "        next_button = response.css(\".next a\")\n",
    "        if next_button:\n",
    "            next_url = next_button.attrib[\"href\"]\n",
    "            next_url = response.urljoin(next_url)\n",
    "            yield scrapy.Request(next_url,\n",
    "                                 callback=self.parse_category,\n",
    "                                 cb_kwargs=dict(cat_name=cat_name))\n",
    "```\n",
    "\n",
    "Finally, let's write the `parse_book` function to scrape each book. For the purposes of this\n",
    "tutorial, we will only scrape the title, price, stock information and star rating. You can use the\n",
    "scrapy shell to build selectors for them. Getting title and price is trivial. The `instock` selector\n",
    "gives a list of strings which mostly consist of spaces and newline with the actual information\n",
    "contained in the middle, thus we use `strip` and `join` to get the required information. Obtaining\n",
    "the star rating is tricky since that information is only contained in the class name, thus we use\n",
    "XPath to get the class name and then select the second word in that. Finally, we return the required\n",
    "information in a dictionary, the reason for this is that scrapy considers these dictionaries as\n",
    "`items`. This allows you to directly export the data to a JSON using `-o output.json` options while\n",
    "running the spider. The main reason to return `items` is to use scrapy pipelines to programmatically\n",
    "process the data. For example, you could write a pipeline to automatically insert the data into\n",
    "a database or write it to a file.\n",
    "\n",
    "### Saving Data using a Pipeline\n",
    "\n",
    "After a spider yields an item, it goes to the item pipeline where each it is processed sequentially\n",
    "by all the pipeline objects. Each object in the pipeline is just a python class that implements\n",
    "a few special functions including `process_item`, `open_spider` (optional) and `close_spider`\n",
    "(optional).\n",
    "\n",
    "Let us now create a pipeline to save the data to a CSV file. We will be using the built-in csv\n",
    "module in python, specifically the `DictWriter` object from that module. Refer the\n",
    "[documentation](https://docs.python.org/3/library/csv.html#csv.DictWriter) for more details.\n",
    "\n",
    "Open `tutorial/pipelines.py` (it would have been auto-generated), delete the existing example\n",
    "pipeline and replace with the following.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "class BookCsvPipeline():\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open(\"output.csv\", \"at\")\n",
    "        fieldnames = [\"title\",\n",
    "                      \"price\",\n",
    "                      \"stock\",\n",
    "                      \"rating\",\n",
    "                      \"category\"]\n",
    "        self.writer = csv.DictWriter(self.file, fieldnames=fieldnames)\n",
    "        if self.file.tell() == 0:\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.writer.writerow(item)\n",
    "        return item\n",
    "```\n",
    "Note that the pipeline's `process_item` needs to return the `item` it processes so that the\n",
    "next processor in the pipeline can process it.\n",
    "\n",
    "Before running the spider, the pipeline that we just created needs to be enabled. Open\n",
    "`tutorial/settings.py` (go through this file once, it contains some very useful options)\n",
    "and look for `ITEM_PIPELINES`, uncomment it and replace with the following.\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'tutorial.pipelines.BookCsvPipeline': 300,\n",
    "}\n",
    "```\n",
    "The `300` indicates its priority in the pipeline with `0` being the highest priority and `1000`\n",
    "being the lowest.\n",
    "\n",
    "Now we are ready to run the spider.\n",
    "```\n",
    "scrapy crawl books -o output.json\n",
    "```\n",
    "This will take a few seconds with lots of output. After it has finished executing, you can find\n",
    "`output.json` and `output.csv` in the same folder from where you ran it. The CSV should contain 163\n",
    "rows if `num_cats_to_parse` was set to 5 (excluding the header).\n",
    "\n",
    "Now you have a scrapy crawler which uses most of the useful functionalities provided by scrapy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnlp37",
   "language": "python",
   "name": "pnlp37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
